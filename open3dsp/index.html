<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A Perception-Planning-Co-Design Framework to Facilitate Robot Task Planning with Open-Vocabulary 3D Scene.">
  <meta name="keywords" content="LLM, Robot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>3DS-Plan</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<style>
  .section {
  margin-bottom: -30px; /* Adjust this value as needed to reduce the space */
}
.expandable-card .card-text-container {
  max-height: 200px;
  overflow-y: hidden;
  position: relative;
}

.expandable-card.expanded .card-text-container {
  max-height: none;
}

.expand-btn {
  position: relative;
  display: none;
  background-color: rgba(255, 255, 255, 0.8);
  /* margin-top: -20px; */
  /* justify-content: center; */
  color: #510c75;
  border-color: transparent;
}

.expand-btn:hover {
  background-color: rgba(200, 200, 200, 0.8);
  text-decoration: none;
  border-color: transparent;
  color: #510c75;
}

.expand-btn:focus {
  outline: none;
  text-decoration: none;
}

.expandable-card:not(.expanded) .card-text-container:after {
  content: "";
  position: absolute;
  bottom: 0;
  left: 0;
  width: 100%;
  height: 90px;
  background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
}

.expandable-card:not(.expanded) .expand-btn {
  margin-top: -40px;
}

.card-body {
  padding-bottom: 5px;
}

.vertical-flex-layout {
  justify-content: center;
  align-items: center;
  height: 100%;
  display: flex;
  flex-direction: column;
  gap: 5px;
}

.figure-img {
  max-width: 100%;
  height: auto;
}

.adjustable-font-size {
  font-size: calc(0.5rem + 2vw);
}

.chat-history {
  flex-grow: 1;
  overflow-y: auto;
  /* overflow-x: hidden; */
  padding: 5px;
  border-bottom: 1px solid #ccc;
  margin-bottom: 10px;
}

#gradio pre {
  background-color: transparent;
}

/* ‰ΩøÁî®Ê∏êÂèòÈ¢úËâ≤ÂÆûÁé∞ÂΩ©ËôπÂ≠ó‰Ωì */
.rainbow-text {
  background: linear-gradient(to right, rgb(255, 53, 201), rgb(18, 227, 250));
  -webkit-background-clip: text;
  color: transparent;
  display: inline-block;
  font-weight: bold;
}


  .video-container {
display: flex;
}

.video-wrapper {
flex: 1;
margin: 10px;
}

.video-wrapper video {
width: 100%;
}
</style>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <!-- <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div> -->
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="rainbow-text">3DS-Plan</span><sup></sup>:
            A Perception-Planning-Co-Design Framework to Facilitate Robot Task Planning with Open-Vocabulary 3D Scene</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Min Xue</a><sup>mxueim@gmail.com</sup>,</span>
            <span class="author-block">
              Jincheng Yu</a><sup></sup>,</span>
            <span class="author-block">
              Lingkun Xiu</a><sup></sup>,
            </span>
            <span class="author-block">
              Jiahao Tang</a><sup></sup>,
            </span><br>
            <span class="author-block">
              Xudong Cai</a><sup></sup>,
            </span>
            <span class="author-block">
              Yali Zhao</a><sup></sup>,
            </span>
            <span class="author-block">
              Shuang Liang</a><sup></sup>,
            </span>
            <span class="author-block">
              Yu Wang</a><sup></sup>
            </span>
  

          </div>

          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block"><sup>1</sup>Tsinghua University</span> -->
            <!-- <span class="author-block"><sup>2</sup>Google Research</span> -->
          </div>

          <!-- <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div> -->
        </div>
      </div>
    </div>
  </div>
</section>

<!-- È¶ñÂõæ -->
<section class="section">
  <div class="container is-max-desktop">
    <p style="text-align:center;">
      <!-- Hero diagram -->
      <img src="./static/images/illustration.png" class="img-responsive" style="width: 400px; height: auto;" />
      <!--/ Hero diagram -->
    </p>
  </div>
</section>




<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">üìù Abstract</h2>
        <div class="content has-text-justified">
          <p>
            To plan and execute robotics tasks, 3D scene perception and understanding are critical for robots to effectively interact with complex environments. Traditional systems often rely on closed vocabularies and are constrained by pre-defined object categories, which limit their flexibility. 
          </p>
          <p>
            In this paper, we propose 3DS-Plan, an open-vocabulary 3D scene perception and generation model designed to facilitate long-horizon task planning. It leverages the pre-trained foundation models to support robotic scene understanding, and further provides environmental details to infer actionable steps in various scenarios. We validate the effectiveness of our framework through comprehensive experiments. It demonstrates a 31.41% improvement in computational efficiency for class operations, and at least a 14.52% increase in success rates for complex tasks, without requiring extensive retraining or extra annotation. The corresponding project page is available at https://techpage.github.io/open3dsp/.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

<!-- youtubeÈìæÊé• -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">üìπ Video</h2>
        <div class="content has-text-justified">
          <!-- <p>
            Using <span class="dnerf">3DS-Plan</span> Perception for Robotic Task Planning.
          </p> -->
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="100%">
            <source src="./static/Video.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!-- <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/kKDDeaSPpYE?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div> -->
      </div>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/llm.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!-- <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div> -->
    </div>
    <!--/ Matting. -->

    <!-- Approach. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">üé® Approach</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">üìñ System Overview</h3>
        <div class="content has-text-justified">
          <p>
            We introduce the 3DS-Plan framework, a novel open-vocabulary 3D scene perception and generation model designed to enhance robotic task planning. This framework integrates pre-trained visual language models, enabling robust and versatile scene understanding in a zero-shot learning setup. In addition, the generated representation of the 3D scene can further provide spatial and environmental details that help in task execution.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <img src="./static/images/overview.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
          <!-- <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div> -->
          <!-- <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div> -->
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Contribution. -->
        <h3 class="title is-4">üìñ Contribution</h3>
        <div class="content has-text-justified">
          <p>
            üí´ We introduce 3DS-Plan, a system that enables task planning and execution based on the perceived environment, without relying on ground truth or predefined maps.
          </p>
          <p>
            üí´ Our system integrates LLM-driven reasoning and common-sense knowledge to generate logical inferences and formulate task plans with actionable steps for execution, which can perform long action sequence tasks.
          </p>
          <p>
            üí´ We demonstrate the effectiveness of our approach through extensive experiments. Our results show improvements in 3D scene generation and success rates for various task types, which validate its applicability in task execution scenarios.
          </p>
        </div>
        <br/>
        <!--/ Contribution. -->

        <!-- Utilization. -->
        <h3 class="title is-4">üìñ Scene Generation for Utilization </h3>
        <div class="content has-text-justified">
          <p>
            3D scene processing and utilization for instance generation and task planning. From capturing input images to integrating instance information for practical task planning, it illustrates a workflow to interpret and respond to scene interaction.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <img src="./static/images/pipeline.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <br/>
        <!--/ Utilization. -->


        <!-- Re-rendering. -->
        <!-- <h3 class="title is-4">Video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">EN-LLM</span>, robots can explore the environment with lauguage model.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/video_compress.mp4"
                    type="video/mp4">
          </video>
        </div> -->
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->

    <!-- Experiment. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">üîç Experiment</h2>

        <!-- Interpolating.  query content --> 
        <!-- <h3 class="title is-4">üìñ 3DS-Plan for Open-vocabulary 3D Scene Understanding</h3>
        <div class="content has-text-justified">
          <p>
            We further delve into the utilization of 3DS-Plan for 3D scene understanding and instance identification from different query perspectives to demonstrate its utility. These queries allow for a diverse analysis of the various elements and their functions.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <img src="./static/images/query.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <br/> -->

        <!-- Interpolating. true data -->
        <!-- <h3 class="title is-4">üìñ Real-World Data for Scene Construction</h3>
        <div class="content has-text-justified">
          <p>
            We have also collected real-world data to reconstruct the environment. It illustrates the results of a scene captured from a realistic environment and processed using 3DS-Plan, which can be perceived with various models such as YOLO, detic to identify objects within the scene.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <img src="./static/images/real_data.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <br/> -->


        <!-- Interpolating. -->
        <h3 class="title is-4">üìñ 3DS-Plan for Task Planning</h3>
        <div class="content has-text-justified">
          <p>
            We conducted our experiments with the AI2-THOR environment and the GPT-3.5 model. A variety of rooms, including kitchens, living rooms, and bedrooms, were selected. We also analyzed the success rates for different task types.
          </p>
          <p>
            In addition, the result also indicates that as the stride increases (thereby reducing environmental data density), the constructed scene becomes less complete, which negatively impacts the success rates across different tasks (complex tasks such as "clean_cool" and "heat_cold" show a significant drop in performance with larger strides). It suggests that denser scene construction is critical for tasks requiring higher action complexity.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <img src="./static/images/sr.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <br/>



        <!-- Utilization. -->
        <h3 class="title is-4">üìñ Demonstration of the Framework for Task Planning </h3>
        <div class="content has-text-justified">
          <p>
            We also generate the corresponding scene instance with 3DS-Plan for the agent, and therefore constitute the environmental foundation for further task planning.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Pick up a bowl and turn a lamp on". It illustrates a simple pick-up task execution process, figure (a) shows a successful task case, as it can infer execution steps accordingly, while figure (b) depicts an attempt using the gpt-neo-1.3b model, which fails to complete the task.
            </p>
            <img src="./static/images/bowl.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Put a heated tomato in the fridge". It requires the agent to find a tomato, heat it in a microwave, and then place it into a specified object. The figure (a) illustrates a successful task planning case, while in figure (b) and figure (c), it doesn't plan out the correct steps due to weaker model. It also shows that our 3DS-Plan can provide an environmental foundation for task planning.
            </p>
            <img src="./static/images/tomato.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <h3 class="title is-4">‚û°Ô∏è More examples can be found as follows</h3>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Place a vase on a coffee table". The figure (a) illustrates a successful task planning case, 
              while figure (b) depicts an attempt using the gpt-neo-1.3b model, which fails to complete the task. figure (c) also shows a successful case with a smaller detection model.
            </p>
            <img src="./static/images/vase.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Place a rinsed plated in the fridge". The figure (a) illustrates a successful task planning case, while in figure (b) and figure (c), it doesn't plan out the correct steps due to weaker model.
            </p>
            <img src="./static/images/plate.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Pick up a laptop and turn on a lamp". The figure (a) illustrates a successful task planning case, 
              while figure (b) depicts an attempt using the gpt-neo-1.3b model, which fails to complete the task. figure (c) also shows a successful case with a smaller detection model.
            </p>
            <img src="./static/images/laptop.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Put a box of tissues on top of the toilet". The figure (a) illustrates a successful task planning case, 
              while figure (b) depicts an attempt using the gpt-neo-1.3b model, which fails to complete the task. figure (c) also shows a successful case with a smaller detection model.
            </p>
            <img src="./static/images/tissue.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <br/>
        <!--/ Utilization. -->





        <!-- Êú¨Âú∞ËßÜÈ¢ë -->
        <!-- Re-rendering. -->
        <!-- <h3 class="title is-4">üìΩÔ∏è Video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">3DS-Plan</span> Perception for Robotic Task Planning.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/Video.mp4"
                    type="video/mp4">
          </video>
        </div> -->


      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <!-- <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p> -->
          <p>
            The website template is borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
