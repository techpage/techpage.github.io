<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Facilitating Robotic Task Planning with Open-Vocabulary 3D Scene Perception.">
  <meta name="keywords" content="LLM, Robot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Open3DSP</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<style>
  .section {
  margin-bottom: -30px; /* Adjust this value as needed to reduce the space */
}
.expandable-card .card-text-container {
  max-height: 200px;
  overflow-y: hidden;
  position: relative;
}

.expandable-card.expanded .card-text-container {
  max-height: none;
}

.expand-btn {
  position: relative;
  display: none;
  background-color: rgba(255, 255, 255, 0.8);
  /* margin-top: -20px; */
  /* justify-content: center; */
  color: #510c75;
  border-color: transparent;
}

.expand-btn:hover {
  background-color: rgba(200, 200, 200, 0.8);
  text-decoration: none;
  border-color: transparent;
  color: #510c75;
}

.expand-btn:focus {
  outline: none;
  text-decoration: none;
}

.expandable-card:not(.expanded) .card-text-container:after {
  content: "";
  position: absolute;
  bottom: 0;
  left: 0;
  width: 100%;
  height: 90px;
  background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
}

.expandable-card:not(.expanded) .expand-btn {
  margin-top: -40px;
}

.card-body {
  padding-bottom: 5px;
}

.vertical-flex-layout {
  justify-content: center;
  align-items: center;
  height: 100%;
  display: flex;
  flex-direction: column;
  gap: 5px;
}

.figure-img {
  max-width: 100%;
  height: auto;
}

.adjustable-font-size {
  font-size: calc(0.5rem + 2vw);
}

.chat-history {
  flex-grow: 1;
  overflow-y: auto;
  /* overflow-x: hidden; */
  padding: 5px;
  border-bottom: 1px solid #ccc;
  margin-bottom: 10px;
}

#gradio pre {
  background-color: transparent;
}

/* ‰ΩøÁî®Ê∏êÂèòÈ¢úËâ≤ÂÆûÁé∞ÂΩ©ËôπÂ≠ó‰Ωì */
.rainbow-text {
  background: linear-gradient(to right, rgb(255, 53, 201), rgb(18, 227, 250));
  -webkit-background-clip: text;
  color: transparent;
  display: inline-block;
  font-weight: bold;
}


  .video-container {
display: flex;
}

.video-wrapper {
flex: 1;
margin: 10px;
}

.video-wrapper video {
width: 100%;
}
</style>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <!-- <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div> -->
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="rainbow-text">Open3DSP</span><sup></sup>:
            Facilitating Robotic Task Planning with Open-Vocabulary 3D Scene Perception</h1>
          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block">
              <a href="https://keunhong.com">Keunhong Park</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://utkarshsinha.com">Utkarsh Sinha</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Jonathan T. Barron</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Sofien Bouaziz</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
            </span> -->
          </div>

          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block"><sup>1</sup>Tsinghua University</span> -->
            <!-- <span class="author-block"><sup>2</sup>Google Research</span> -->
          </div>

          <!-- <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div> -->
        </div>
      </div>
    </div>
  </div>
</section>

<!-- È¶ñÂõæ -->
<section class="section">
  <div class="container is-max-desktop">
    <p style="text-align:center;">
      <!-- Hero diagram -->
      <img src="./static/images/illustration.png" class="img-responsive" style="width: 400px; height: auto;" />
      <!--/ Hero diagram -->
    </p>
  </div>
</section>




<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">üìù Abstract</h2>
        <div class="content has-text-justified">
          <p>
            3D scene perception and understanding are critical for robots to interact effectively with complex environments. Traditional systems often rely on closed vocabularies and are constrained by pre-defined object categories, therefore limiting flexibility and adaptability.  
          </p>
          <p>
            In this paper, we propose Open3DSP, an open-vocabulary 3D scene perception and generation model that leverages pre-trained foundation models to support robotic scene understanding, and further provide spatial and environmental details that help in task planning. Besides, it allows the robot to grasp not only the geometric characteristics of spaces but also their functional properties, assisting the execution of complex tasks in an unstructured environment. We validate the effectiveness of our framework through comprehensive experiments, it demonstrates an improvement in mAcc and F-mIoU while reducing the time spent on class calculations by 31.41%. Additionally, it enables the robot to handle complex environments for inferential tasks without requiring extensive retraining or manual annotation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

<!-- youtubeÈìæÊé• -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">üìπ Youtube Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/7cT9FhE-9uw?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/llm.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!-- <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div> -->
    </div>
    <!--/ Matting. -->

    <!-- Approach. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">üé® Approach</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">üìñ System Overview</h3>
        <div class="content has-text-justified">
          <p>
            We introduce the Open3DSP framework, a novel open-vocabulary 3D scene perception and generation model designed to enhance robotic task planning. This framework integrates pre-trained visual language models, enabling robust and versatile scene understanding in a zero-shot learning setup. In addition, the generated representation of the 3D scene can further provide spatial and environmental details that help in task execution.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <img src="./static/images/overview.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
          <!-- <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div> -->
          <!-- <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div> -->
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Contribution. -->
        <h3 class="title is-4">üìñ Contribution</h3>
        <div class="content has-text-justified">
          <p>
            üí´ Utilization of perceived information in 3D scene generation for robot task planning:  we propose Open3DSP, a framework for open-vocabulary 3D scene perception and generation with pre-trained language model that enable further task-specific action.
          </p>
          <p>
            üí´ Open3DSP leverages the common-sense knowledge embedded in large language models and detection labels to refine instance feature, and further validates the effectiveness in 3D scene generation.
          </p>
          <p>
            üí´ We introduce a pipeline to handle inferential tasks within 3D scene, and enable the robot to operate in unstructured environments, it also shows versatility in 3D scene construction and application.
          </p>
        </div>
        <br/>
        <!--/ Contribution. -->

        <!-- Utilization. -->
        <h3 class="title is-4">üìñ Scene Generation for Utilization </h3>
        <div class="content has-text-justified">
          <p>
            3D scene processing and utilization for instance generation and task planning. From capturing input images to integrating instance information for practical task planning, it illustrates a workflow to interpret and respond to scene interaction.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <img src="./static/images/map_generation.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <br/>
        <!--/ Utilization. -->


        <!-- Re-rendering. -->
        <!-- <h3 class="title is-4">Video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">EN-LLM</span>, robots can explore the environment with lauguage model.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/video_compress.mp4"
                    type="video/mp4">
          </video>
        </div> -->
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->

    <!-- Experiment. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">üîç Experiment</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">üìñ Open3DSP for Open-vocabulary 3D Scene Understanding</h3>
        <div class="content has-text-justified">
          <p>
            We further delve into the utilization of Open3DSP for 3D scene understanding and instance identification from different query perspectives to demonstrate its utility. These queries allow for a diverse analysis of the various elements and their functions.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <img src="./static/images/query.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
          <!-- <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div> -->
          <!-- <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div> -->
        </div>
        <br/>
        <!--/ Interpolating. -->


        <!-- Utilization. -->
        <h3 class="title is-4">üìñ 3D Scene Generation for Task Planning </h3>
        <div class="content has-text-justified">
          <p>
            We also generate the corresponding scene instance with Open3DSP for the agent, and therefore constitute the environmental foundation for further task planning.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Pick up a bowl and turn a lamp on". It illustrates a simple pick-up task execution process, figure (a) shows a successful task case, as it can infer execution steps accordingly, while figure (b) depicts an attempt using the OPT-1.3b model, which fails to complete the task.
            </p>
            <img src="./static/images/bowl.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Put a heated tomato in the fridge". It requires the agent to find a tomato, heat it in a microwave, and then place it into a specified object. The figure (a) illustrates a successful task planning case, while in figure (b) and figure (c), it doesn't plan out the correct steps due to weaker model. It also shows that our Open3DSP can provide an environmental foundation for task planning.
            </p>
            <img src="./static/images/tomato.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <h3 class="title is-4">‚û°Ô∏è More examples can be found as follows</h3>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Place a vase on a coffee table". The figure (a) illustrates a successful task planning case, 
              while figure (b) depicts an attempt using the OPT-1.3b model, which fails to complete the task. figure (c) also shows a successful case with a smaller detection model.
            </p>
            <img src="./static/images/vase.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Place a rinsed plated in the fridge". The figure (a) illustrates a successful task planning case, while in figure (b) and figure (c), it doesn't plan out the correct steps due to weaker model.
            </p>
            <img src="./static/images/plate.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Pick up a laptop and turn on a lamp". The figure (a) illustrates a successful task planning case, 
              while figure (b) depicts an attempt using the OPT-1.3b model, which fails to complete the task. figure (c) also shows a successful case with a smaller detection model.
            </p>
            <img src="./static/images/laptop.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Put a box of tissues on top of the toilet". The figure (a) illustrates a successful task planning case, 
              while figure (b) depicts an attempt using the OPT-1.3b model, which fails to complete the task. figure (c) also shows a successful case with a smaller detection model.
            </p>
            <img src="./static/images/tissue.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <br/>
        <!--/ Utilization. -->





        <!-- Êú¨Âú∞ËßÜÈ¢ë -->
        <!-- Re-rendering. -->
        <h3 class="title is-4">üìΩÔ∏è Video</h3>
        <div class="content has-text-justified">
          <!-- <p>
            Using <span class="dnerf">Open3DSP</span> Perception for Robotic Task Planning.
          </p> -->
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/video_compress.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Re-rendering. -->
        <!--/ Êú¨Âú∞ËßÜÈ¢ë -->

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <!-- <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p> -->
          <p>
            The website template is borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
