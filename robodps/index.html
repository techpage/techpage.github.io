<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A Unified Robot Data Management Platform for
        Manipulation and Dynamic Task Scheduling.">
  <meta name="keywords" content="LLM, Robot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>robodps</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<style>
  .section {
  margin-bottom: -30px; /* Adjust this value as needed to reduce the space */
}
.expandable-card .card-text-container {
  max-height: 200px;
  overflow-y: hidden;
  position: relative;
}

.expandable-card.expanded .card-text-container {
  max-height: none;
}

.expand-btn {
  position: relative;
  display: none;
  background-color: rgba(255, 255, 255, 0.8);
  /* margin-top: -20px; */
  /* justify-content: center; */
  color: #510c75;
  border-color: transparent;
}

.expand-btn:hover {
  background-color: rgba(200, 200, 200, 0.8);
  text-decoration: none;
  border-color: transparent;
  color: #510c75;
}

.expand-btn:focus {
  outline: none;
  text-decoration: none;
}

.expandable-card:not(.expanded) .card-text-container:after {
  content: "";
  position: absolute;
  bottom: 0;
  left: 0;
  width: 100%;
  height: 90px;
  background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
}

.expandable-card:not(.expanded) .expand-btn {
  margin-top: -40px;
}

.card-body {
  padding-bottom: 5px;
}

.vertical-flex-layout {
  justify-content: center;
  align-items: center;
  height: 100%;
  display: flex;
  flex-direction: column;
  gap: 5px;
}

.figure-img {
  max-width: 100%;
  height: auto;
}

.adjustable-font-size {
  font-size: calc(0.5rem + 2vw);
}

.chat-history {
  flex-grow: 1;
  overflow-y: auto;
  /* overflow-x: hidden; */
  padding: 5px;
  border-bottom: 1px solid #ccc;
  margin-bottom: 10px;
}

#gradio pre {
  background-color: transparent;
}

/* ‰ΩøÁî®Ê∏êÂèòÈ¢úËâ≤ÂÆûÁé∞ÂΩ©ËôπÂ≠ó‰Ωì */
.rainbow-text {
  background: linear-gradient(to right, rgb(255, 53, 201), rgb(18, 227, 250));
  -webkit-background-clip: text;
  color: transparent;
  display: inline-block;
  font-weight: bold;
}


  .video-container {
display: flex;
}

.video-wrapper {
flex: 1;
margin: 10px;
}

.video-wrapper video {
width: 100%;
}
</style>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <!-- <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div> -->
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="rainbow-text">RoboDPS</span><sup></sup>:
            A Unified Robot Data Management Platform for Manipulation with Dynamic Multi-Task Scheduling</h1>
          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block">
              Min Xue</a><sup></sup>,</span>
            <span class="author-block">
              Shuaiwen Chen</a><sup></sup>,</span>
            <span class="author-block">
              Jinghao Zhou</a><sup></sup>,
            </span>
            <span class="author-block">
              Tianxing Li</a><sup></sup>,
            </span>
            <span class="author-block">
              Nengbin Li</a><sup></sup>,
            </span>
            <span class="author-block">
              Jian Wang</a><sup></sup>,
            </span>
            <span class="author-block">
              Jincheng Yu</a><sup>*</sup>,
            </span>
            <span class="author-block">
              Yu Wang</a><sup>*</sup>
            </span> -->

            <!-- <span class="author-block">
              <a href="https://keunhong.com">Keunhong Park</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://utkarshsinha.com">Utkarsh Sinha</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Jonathan T. Barron</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Sofien Bouaziz</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
            </span> -->
          </div>

          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block"><sup>1</sup>Tsinghua University</span>
            <span class="author-block"><sup>2</sup>Google Research</span> -->
          </div>

          <!-- <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div> -->
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h5 class="title is-5">üë©‚Äçüíª We are currently deploying the public version of the robot data platform, and will soon provide trial accounts for some features, along with a Docker deployment option once the setup is complete.</h5> -->
        <h5 class="title is-5">üë©‚Äçüíª We have now provided trial accounts (please <a href="https://github.com/techpage/techpage.github.io/blob/main/robodps/trial_account.md" target="_blank">click here</a>) for some features as we continue deploying the public version of the robot data platform. A Docker deployment option will also be available once the setup is complete.</h5>
        <h6 class="title is-6">‚òëÔ∏è 25.03.17: We have updated the documentation for the trial website.</h6>
        <div class="content has-text-justified">
        </div>
      </div>
    </div>
  </div>

<!-- We are currently deploying the public version of the robot data platform on Alibaba Cloud servers, and will release the RoboDPS once the setup is complete. -->



<!-- È¶ñÂõæ -->
<section class="section">
  <div class="container is-max-desktop">
    <p style="text-align:center;">
      <!-- Hero diagram -->
      <img src="./static/images/illustration.png" class="img-responsive" style="width: 700px; height: auto;" />
      <!--/ Hero diagram -->
    </p>
  </div>
</section>



<!-- Ê®™ÂêëÊªöÂä®ÊòæÁ§∫ËßÜÈ¢ë -->
<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/scene0.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/scene1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/scene2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/board3.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <!-- <div class="container is-max-desktop"> -->
  <div class="column">
  <div class="content">
<div style="display: flex; flex-direction: column; align-items: center; width: 100%; margin-top: 2em;">
  <div style="display: flex; justify-content: space-around; align-items: center;margin-bottom: 2em; width: 100%;">
    <div style="display: flex; justify-content: space-around; align-items: center;width: 100%;">
      <div style="text-align: center; width: 100%;">
          <video controls autoplay muted loop style="width: 100%;">
              <source src="./static/videos/sim/scene0.mp4" type="video/mp4">
              Your browser does not support the video tag.
          </video>
      </div>
    </div>
    <div style="display: flex; justify-content: space-around; align-items: center;width: 100%;">
      <div style="text-align: center; width: 100%;">
          <video controls autoplay muted loop style="width: 100%;">
              <source src="./static/videos/sim/scene1.mp4" type="video/mp4">
              Your browser does not support the video tag.
          </video>
      </div>
    </div>
    <div style="display: flex; justify-content: space-around; align-items: center;width: 100%;">
      <div style="text-align: center; width: 100%;">
          <video controls autoplay muted loop style="width: 100%;">
              <source src="./static/videos/sim/scene2.mp4" type="video/mp4">
              Your browser does not support the video tag.
          </video>
      </div>
    </div>
  </div>
</div>
</div>
</div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2">üìù Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The deployment of robots in industrial and household applications has expanded significantly in recent years. However, the increasing complexity and diversity of tasks have led to challenges in managing the vast amounts of data generated by robots, including perception information, body state, and task execution status, etc. Effective utilization of this data is hindered by issues related to data management and adaptation. To address these challenges, in this paper, we propose a unified robot data management platform that effectively stores, retrieves, and visualizes data while reducing adaptation costs. The platform is designed to support flexible and customizable data structures, and facilitate the extraction of task-relevant information. Furthermore, to tackle the inconsistent success rates of manipulation tasks caused by factors such as task complexity, environmental uncertainties, and manipulation accuracy, we construct a "data platform - policy generation - task scheduling" pipeline to demonstrate the platform's application in robot manipulation, and our dynamic task scheduling mechanism can achieve a time saving of approximately 25% to enhance system efficiency. The corresponding project page is available at https://techpage.github.io/robodps/.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

<!-- youtubeÈìæÊé• -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2">üìπ YouTube Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/yzqX6BTbmwQ?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
        <h2 class="title is-2">üìπ Data Platform Clip</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/miIClEE_Ing?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
  </div> -->


        <!-- Êú¨Âú∞ËßÜÈ¢ë -->
        <!-- Re-rendering. -->

        <h2 class="title is-2">üìΩÔ∏è RoboDPS Video</h2>
        <div class="content has-text-justified">
          <!-- <p>
            Using <span class="dnerf">3DS-Plan</span> Perception for Robotic Task Planning.
          </p> -->
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/video_git.mp4"
                    type="video/mp4">
          </video>
        </div>
        <h2 class="title is-2">üìΩÔ∏è Data Platform Clip</h2>
        <div class="content has-text-justified">
          <!-- <p>
            Using <span class="dnerf">3DS-Plan</span> Perception for Robotic Task Planning.
          </p> -->
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/clip_git.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Re-rendering. -->
        <!--/ Êú¨Âú∞ËßÜÈ¢ë -->
      </div>
    </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/llm.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!-- <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div> -->
    </div>
    <!--/ Matting. -->

    <!-- Approach. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2">üé® Approach</h2>

        <!-- Interpolating. -->
        <h3 class="title is-3">üìñ System Overview</h3>
        <div class="content has-text-justified">
          <p>
            To manage data from various sources and formats, it is essential to parse and unify the collected data, which also helps to reduce the adaptation complexity for downstream tasks. In this section, we first introduce RooboDPS and then provide a usage example with grasping tasks.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <img src="./static/images/overview.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
          <!-- <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div> -->
          <!-- <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div> -->
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Contribution. -->
        <h3 class="title is-3">üìñ Contribution</h3>
        <div class="content has-text-justified">
          <p>
            üí´ To address the challenges of managing multi-source heterogeneous data, we introduce a unified robot data platform with support for flexible and customizable data. This enables efficient storage and parsing of embodied robot data while mitigating the complexity caused by non-uniform data structures.
          </p>
          <p>
            üí´ The platform supports multiple retrieval ways and provides an intuitive visualization interface, allowing users to effectively extract relevant data through query statements, thereby enhancing task performance.
          </p>
          <p>
            üí´ Taking the task success rate into consideration, we introduce a dynamic task scheduling framework that incorporates fault-tolerant re-planning, which achieves a time saving of approximately 25%. Furthermore, by constructing a robot "data platform - policy generation - task scheduling" pipeline, we demonstrate the application of the data platform in task manipulation.
          </p>
        </div>
        <br/>
        <!--/ Contribution. -->

        <!-- Utilization. -->
        <!-- <h3 class="title is-4">üìñ Scene Generation for Utilization </h3>
        <div class="content has-text-justified">
          <p>
            3D scene processing and utilization for instance generation and task planning. From capturing input images to integrating instance information for practical task planning, it illustrates a workflow to interpret and respond to scene interaction.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <img src="./static/images/pipeline.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <br/> -->
        <!--/ Utilization. -->


        <!-- Re-rendering. -->
        <!-- <h3 class="title is-4">Video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">EN-LLM</span>, robots can explore the environment with lauguage model.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/video_compress.mp4"
                    type="video/mp4">
          </video>
        </div> -->
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->

    <!-- Experiment. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2">üîç Application of RoboDPS for Robot Manipulation</h2>

        <!-- Interpolating.  query content --> 
        <!-- <h3 class="title is-4">üìñ 3DS-Plan for Open-vocabulary 3D Scene Understanding</h3>
        <div class="content has-text-justified">
          <p>
            We further delve into the utilization of 3DS-Plan for 3D scene understanding and instance identification from different query perspectives to demonstrate its utility. These queries allow for a diverse analysis of the various elements and their functions.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <img src="./static/images/query.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <br/> -->

        <!-- Interpolating. true data -->
        <!-- <h3 class="title is-4">üìñ Real-World Data for Scene Construction</h3>
        <div class="content has-text-justified">
          <p>
            We have also collected real-world data to reconstruct the environment. It illustrates the results of a scene captured from a realistic environment and processed using 3DS-Plan, which can be perceived with various models such as YOLO, detic to identify objects within the scene.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <img src="./static/images/real_data.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <br/> -->


          <!-- Visual Effects. -->
          <div class="column">
            <div class="content">
              <h3 class="title is-3">üìñ Simulation</h3>
              <h4 class="title is-4">‚û°Ô∏è Mainboard Task</h4>
              <p>
                When trained solely on <i>maninboard</i> 1, the model can generalize to unseen <i>mainboard</i> 2 and 3, with success rates of 0.45 and 0.35, respectively, after 300 epochs. However, the model fails to manipulate the wooden board, as its appearance closely resembles that of the box placed nearby. This similarity in visual features between the manipulated object and the placed object has misinterpret the scene, and thus being unable to perform the task accurately. By incorporating additional training data from the <i>mainboard</i> 3, the model achieves a success rate of 45% on wooden board, demonstrating that mixed training data can improve generalization across different objects.
              </p>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/sim/board0.mp4"
                        type="video/mp4">
              </video>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/sim/board1.mp4"
                        type="video/mp4">
              </video>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/sim/board2.mp4"
                        type="video/mp4">
              </video>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/sim/board3.mp4"
                        type="video/mp4">
              </video>
            </div>
          </div>

          <div class="column">
            <div class="content">
              <h3 class="title is-3">‚û°Ô∏è Wooden Lid Task</h3>
              <p>
                To evaluate the effect of visual similarity between the grasped and target objects, we conduct an ablation experiment with the second task, where both objects have similar visual textures and colors. When using <i>lid</i> 1 for training, the results indicate that the success rates remained low across all lids, even with increased training epochs. Furthermore, even when the data from <i>mainboard</i> 3 is included in the training process, the semantic confusion caused by these wooden objects still prevents the model from executing the corresponding grasping actions.
              </p>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/sim/lid0.mp4"
                        type="video/mp4">
              </video>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/sim/lid1.mp4"
                        type="video/mp4">
              </video>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/sim/lid2.mp4"
                        type="video/mp4">
              </video>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/sim/lid3.mp4"
                        type="video/mp4">
              </video>
            </div>
          </div>

          <div class="column">
            <div class="content">
              <h3 class="title is-3">‚û°Ô∏è Peg Insertion Task</h3>
              <p>
                For the peg insertion task, training on <i>peg</i> 1 with 80 episodes results in a success rate of 0.8 at epoch 300, with generalization to other pegs and success rates ranging from 0.7-0.85.
              </p>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/sim/plug0.mp4"
                        type="video/mp4">
              </video>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/sim/plug1.mp4"
                        type="video/mp4">
              </video>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/sim/plug2.mp4"
                        type="video/mp4">
              </video>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/sim/plug3.mp4"
                        type="video/mp4">
              </video>
            </div>
          </div>
          
          <div class="column">
            <div class="content">
              <h3 class="title is-3">‚û°Ô∏è Tube Insertion Task</h3>
              <p>
                For tube insertion task, the model trained with <i>tube</i> 1 yields a success rate of 0.6 at epoch 300, generalization to unseen <i>tube</i> 2-4 achieves success rate of 0.5-0.65. In addition, the model can also be trained with mixed data.
              </p>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/sim/bolt0.mp4"
                        type="video/mp4">
              </video>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/sim/bolt1.mp4"
                        type="video/mp4">
              </video>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/sim/bolt2.mp4"
                        type="video/mp4">
              </video>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/sim/bolt3.mp4"
                        type="video/mp4">
              </video>
            </div>
          </div>          
          <!--/ Visual Effects. -->



          <!-- Visual Effects. -->
          <div class="column">
            <div class="content">
              <h3 class="title is-3">üìñ Real-World Manipulation</h3>
              <p>
                The data retrieved from the RoboDPS can be used for further training. In this section, to illustrate the utilization of RoboDPS, we provide an application example in a real-world environment.
              </p>
              <h4 class="title is-4">‚û°Ô∏è Put Candies of Different Colors into the Box</h4>
              <!-- <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/real/r.mp4"
                        type="video/mp4">
              </video>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/real/p.mp4"
                        type="video/mp4">
              </video>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/real/y.mp4"
                        type="video/mp4">
              </video> -->
              <section class="section">
                <div class="container is-max-desktop">
                <div class="column">
                <div class="content">
              <div style="display: flex; flex-direction: column; align-items: center; width: 100%; margin-top: 2em;">
                <div style="display: flex; justify-content: space-around; align-items: center;margin-bottom: 2em; width: 100%;">
                  <div style="display: flex; justify-content: space-around; align-items: center;width: 100%;">
                    <div style="text-align: center; width: 100%;">
                        <video controls autoplay muted loop style="width: 100%;">
                            <source src="./static/videos/real/r.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                  </div>
                  <div style="display: flex; justify-content: space-around; align-items: center;width: 100%;">
                    <div style="text-align: center; width: 100%;">
                        <video controls autoplay muted loop style="width: 100%;">
                            <source src="./static/videos/real/p.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                  </div>
                  <div style="display: flex; justify-content: space-around; align-items: center;width: 100%;">
                    <div style="text-align: center; width: 100%;">
                        <video controls autoplay muted loop style="width: 100%;">
                            <source src="./static/videos/real/y.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                  </div>
                </div>
              </div>
              </div>
              </div>
              </section>
              <h4 class="title is-4">‚û°Ô∏è Put Candies into the plate</h4>
              <!-- <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/real/r1.mp4"
                        type="video/mp4">
              </video>
              <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/real/y1.mp4"
                        type="video/mp4">
              </video> -->
              <section class="section">
                <div class="container is-max-desktop">
                <div class="column">
                <div class="content">
              <div style="display: flex; flex-direction: column; align-items: center; width: 100%; margin-top: 2em;">
                <div style="display: flex; justify-content: space-around; align-items: center;margin-bottom: 2em; width: 100%;">
                  <div style="display: flex; justify-content: space-around; align-items: center;width: 100%;">
                    <div style="text-align: center; width: 100%;">
                        <video controls autoplay muted loop style="width: 100%;">
                            <source src="./static/videos/real/r1.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                  </div>
                  <div style="display: flex; justify-content: space-around; align-items: center;width: 100%;">
                    <div style="text-align: center; width: 100%;">
                        <video controls autoplay muted loop style="width: 100%;">
                            <source src="./static/videos/real/y1.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                  </div>
                </div>
              </div>
              </div>
              </div>
              </section>
            </div>
          </div>





        <!-- Utilization. -->
        <!-- <h3 class="title is-3">üìñ Demonstration of the Framework for Task Planning </h3>
        <div class="content has-text-justified">
          <p>
            We also generate the corresponding scene instance with 3DS-Plan for the agent, and therefore constitute the environmental foundation for further task planning.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Pick up a bowl and turn a lamp on". It illustrates a simple pick-up task execution process, figure (a) shows a successful task case, as it can infer execution steps accordingly, while figure (b) depicts an attempt using the gpt-neo-1.3b model, which fails to complete the task.
            </p>
            <img src="./static/images/bowl.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Put a heated tomato in the fridge". It requires the agent to find a tomato, heat it in a microwave, and then place it into a specified object. The figure (a) illustrates a successful task planning case, while in figure (b) and figure (c), it doesn't plan out the correct steps due to weaker model. It also shows that our 3DS-Plan can provide an environmental foundation for task planning.
            </p>
            <img src="./static/images/tomato.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <h3 class="title is-3">‚û°Ô∏è More examples can be found as follows</h3>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Place a vase on a coffee table". The figure (a) illustrates a successful task planning case, 
              while figure (b) depicts an attempt using the gpt-neo-1.3b model, which fails to complete the task. figure (c) also shows a successful case with a smaller detection model.
            </p>
            <img src="./static/images/vase.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Place a rinsed plated in the fridge". The figure (a) illustrates a successful task planning case, while in figure (b) and figure (c), it doesn't plan out the correct steps due to weaker model.
            </p>
            <img src="./static/images/plate.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Pick up a laptop and turn on a lamp". The figure (a) illustrates a successful task planning case, 
              while figure (b) depicts an attempt using the gpt-neo-1.3b model, which fails to complete the task. figure (c) also shows a successful case with a smaller detection model.
            </p>
            <img src="./static/images/laptop.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-centered has-text-centered">
            <p>
              üè∑Ô∏è Task planning for "Put a box of tissues on top of the toilet". The figure (a) illustrates a successful task planning case, 
              while figure (b) depicts an attempt using the gpt-neo-1.3b model, which fails to complete the task. figure (c) also shows a successful case with a smaller detection model.
            </p>
            <img src="./static/images/tissue.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
        <br/> -->
        <!--/ Utilization. -->








    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <!-- <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p> -->
          <p>
            The website template is borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
